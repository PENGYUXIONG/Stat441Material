diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 6 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
plot(prune.tree)
text(diabetes.tree, pretty = 0)
plot(prune.tree)
text(diabetes.tree, pretty = 0)
setwd("./Desktop/stat 441/Assignment4/")
library("tree")
library("e1071")
library("gbm")
# data processing
diabetes.data = read.table("diabetes.data", header=T,na.strings=c(""))
cols_to_imp = c("plas","pres","skin","insu","mass", "pedi", "age")
diabetes.data[cols_to_imp][diabetes.data[cols_to_imp] == 0] = NA
diabetes_col = colMeans(diabetes.data[, cols_to_imp], na.rm=TRUE)
for(i in 1:ncol(diabetes.data)) {
diabetes.data[ , i][is.na(diabetes.data[ , i])] = as.integer(mean(diabetes.data[ , i], na.rm = TRUE))
}
# cross validation
# validation size = 168 training size = 600
# randomly select 100 samples randomly for each test batch, 10 batches in total.
# choose the batch that has best performance
set.seed(1)
train.index = sample (1: nrow(diabetes.data ), 600)
diabetes.train = diabetes.data[train.index, ]
diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 8 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
plot(prune.tree)
text(diabetes.tree, pretty = 0)
text(diabetes.tree, pretty = 1)
plot(prune.tree)
text(diabetes.tree, pretty = 1)
plot(prune.tree)
text(diabetes.tree, pretty = 5)
setwd("./Desktop/stat 441/Assignment4/")
library("tree")
library("e1071")
library("gbm")
# data processing
diabetes.data = read.table("diabetes.data", header=T,na.strings=c(""))
cols_to_imp = c("plas","pres","skin","insu","mass", "pedi", "age")
diabetes.data[cols_to_imp][diabetes.data[cols_to_imp] == 0] = NA
diabetes_col = colMeans(diabetes.data[, cols_to_imp], na.rm=TRUE)
for(i in 1:ncol(diabetes.data)) {
diabetes.data[ , i][is.na(diabetes.data[ , i])] = as.integer(mean(diabetes.data[ , i], na.rm = TRUE))
}
# cross validation
# validation size = 168 training size = 600
# randomly select 100 samples randomly for each test batch, 10 batches in total.
# choose the batch that has best performance
set.seed(1)
train.index = sample (1: nrow(diabetes.data ), 600)
diabetes.train = diabetes.data[train.index, ]
diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 10 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
plot(prune.tree)
text(diabetes.tree, pretty = 5)
setwd("./Desktop/stat 441/Assignment4/")
library("tree")
library("e1071")
library("gbm")
# data processing
diabetes.data = read.table("diabetes.data", header=T,na.strings=c(""))
cols_to_imp = c("plas","pres","skin","insu","mass", "pedi", "age")
diabetes.data[cols_to_imp][diabetes.data[cols_to_imp] == 0] = NA
diabetes_col = colMeans(diabetes.data[, cols_to_imp], na.rm=TRUE)
for(i in 1:ncol(diabetes.data)) {
diabetes.data[ , i][is.na(diabetes.data[ , i])] = as.integer(mean(diabetes.data[ , i], na.rm = TRUE))
}
# cross validation
# validation size = 168 training size = 600
# randomly select 100 samples randomly for each test batch, 10 batches in total.
# choose the batch that has best performance
set.seed(1)
train.index = sample (1: nrow(diabetes.data ), 600)
diabetes.train = diabetes.data[train.index, ]
diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 12 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
plot(prune.tree)
text(diabetes.tree, pretty = 5)
prune.tree =prune.misclass (diabetes.tree, best = 8 )
plot(prune_tree)
plot(prune.tree)
text(prune.tree)
prune.tree =prune.misclass (diabetes.tree, best = 6 )
plot(prune_tree)
plot(prune.tree)
text(prune.tree)
mean((yhat.boost - diabetes.val$positive)^2)
summary(diabetes.boost)
setwd("./Desktop/stat 441/Assignment4/")
library("tree")
library("e1071")
library("gbm")
# data processing
diabetes.data = read.table("diabetes.data", header=T,na.strings=c(""))
cols_to_imp = c("plas","pres","skin","insu","mass", "pedi", "age")
diabetes.data[cols_to_imp][diabetes.data[cols_to_imp] == 0] = NA
diabetes_col = colMeans(diabetes.data[, cols_to_imp], na.rm=TRUE)
for(i in 1:ncol(diabetes.data)) {
diabetes.data[ , i][is.na(diabetes.data[ , i])] = as.integer(mean(diabetes.data[ , i], na.rm = TRUE))
}
# cross validation
# validation size = 168 training size = 600
# randomly select 100 samples randomly for each test batch, 10 batches in total.
# choose the batch that has best performance
set.seed(1)
train.index = sample (1: nrow(diabetes.data ), 600)
diabetes.train = diabetes.data[train.index, ]
diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 8 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
setwd("./Desktop/stat 441/Assignment4/")
library("tree")
library("e1071")
library("gbm")
# data processing
diabetes.data = read.table("diabetes.data", header=T,na.strings=c(""))
cols_to_imp = c("plas","pres","skin","insu","mass", "pedi", "age")
diabetes.data[cols_to_imp][diabetes.data[cols_to_imp] == 0] = NA
diabetes_col = colMeans(diabetes.data[, cols_to_imp], na.rm=TRUE)
for(i in 1:ncol(diabetes.data)) {
diabetes.data[ , i][is.na(diabetes.data[ , i])] = as.integer(mean(diabetes.data[ , i], na.rm = TRUE))
}
# cross validation
# validation size = 168 training size = 600
# randomly select 100 samples randomly for each test batch, 10 batches in total.
# choose the batch that has best performance
set.seed(1)
train.index = sample (1: nrow(diabetes.data ), 600)
diabetes.train = diabetes.data[train.index, ]
diabetes.val = diabetes.data[-train.index, ]
# simple tree classification
diabetes.tree = tree(as.factor(positive)~. -positive, diabetes.data, subset = train.index)
plot(diabetes.tree)
text(diabetes.tree, pretty = 0)
# tree pruning block using simple tree model
prune.tree =prune.misclass (diabetes.tree, best = 8 )
# cross validation
# k-fold 10 epochs
classify_tree_current_acc = 0
prune_tree_current_acc = 0
for (t in 1:10){
# set batch size to 100
val_batch_index = sample(1:nrow(diabetes.val), 100)
val_batch = diabetes.val[val_batch_index, ]
tree.pred=predict(diabetes.tree, val_batch,type ="class")
prune_tree.pred = predict(prune.tree, val_batch, type = "class")
val_batch.test = val_batch[, 9]
table(tree.pred, val_batch.test)
classify_tree_current_acc = mean(tree.pred == val_batch.test) + classify_tree_current_acc
table(prune_tree.pred, val_batch.test)
prune_tree_current_acc = mean(prune_tree.pred == val_batch.test) + prune_tree_current_acc
}
actual_classify_tree_acc = classify_tree_current_acc / 10
actual_prune_tree_acc = prune_tree_current_acc / 10
# boosting model
set.seed(1)
diabetes.boost = gbm(positive~.,data=diabetes.train, shrinkage=0.01, distribution= "bernoulli"
,n.trees =6000 , interaction.depth = 4, cv.folds = 10, verbose = F)
best.iter = gbm.perf(diabetes.boost, method="cv")
summary(diabetes.boost)
yhat.boost = predict(diabetes.boost, newdata = diabetes.val, n.trees = 6000, type = "link")
mean((yhat.boost - diabetes.val$positive)^2)
prediction_binary = as.factor(ifelse(yhat.boost > 0.5, 1, 0 ))
ground_T = as.factor(diabetes.val$positive)
boosting_acc = mean(prediction_binary == ground_T)
# svm Model
tune.out= tune(svm, positive~. ,data=diabetes.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
svmfit = svm( positive~., data=diabetes.train , type = "C-classification", kernel ="linear", cost =0.01)
prediction = predict(svmfit, diabetes.val[, -9])
svm_acc = mean(prediction == diabetes.val$positive)
plot(svmfit, diabetes.val, mass~age)
best.iter = gbm.perf(diabetes.boost, method="cv")
plot(svmfit, diabetes.val, mass~age)
plot(svmfit, diabetes.val, mass~plas)
setwd("./Desktop/stat 441/Assignment4/")
library("rpart")
library("ipred")
library("tree")
library("e1071")
library("gbm")
oil.data = read.table("crude_oil.data", header=T,na.strings=c(""))
# data size is only 56, split into test and training set
set.seed(1)
train.index = sample (1: nrow(oil.data ), 40)
oil.train = oil.data[train.index, ]
oil.test = oil.data[-train.index, ]
# simple tree classification
oil.tree = tree(as.factor(unit)~. , oil.data, subset = train.index)
# classification tree model has 4 nodes
plot(oil.tree)
text(oil.tree, pretty = 0)
# tree pruning
prune.tree =prune.misclass (oil.tree ,best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred=predict(oil.tree, oil.test,type ="class")
prune_tree.pred = predict(prune.tree, oil.test, type = "class")
Classifcation_tree_acc = mean(tree.pred == oil.test$unit)
prune_tree_acc = mean(prune_tree.pred == oil.test$unit)
# bagging
oil.bagging = bagging(unit~. , data = oil.train,nbagg = 100, coob = T,
control = rpart.control(minsplit = 2), cp = 0)
oil_bag_pred = predict(oil.bagging, newdata = oil.test)
mean(oil.test$unit == oil_bag_pred)
# svm model
tune.out= tune(svm, unit~. ,data=oil.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
# best cost = 1
svmfit = svm( unit~., data=oil.train , type = "C-classification", kernel ="linear", cost =1)
svm_prediction = predict(svmfit, oil.test[, -6])
svm_acc = mean(prediction == oil.test$unit)
plot(svmfit, oil.test, x1~x2)
plot(oil.tree)
text(oil.tree)
plot(oil.bagging)
setwd("./Desktop/stat 441/Assignment4/")
library("rpart")
library("ipred")
library("tree")
library("e1071")
library("gbm")
oil.data = read.table("crude_oil.data", header=T,na.strings=c(""))
# data size is only 56, split into test and training set
set.seed(1)
train.index = sample (1: nrow(oil.data ), 40)
oil.train = oil.data[train.index, ]
oil.test = oil.data[-train.index, ]
# simple tree classification
oil.tree = tree(as.factor(unit)~. , oil.data, subset = train.index)
# classification tree model has 4 nodes
plot(oil.tree)
text(oil.tree, pretty = 0)
# tree pruning
prune.tree =prune.misclass (oil.tree ,best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred=predict(oil.tree, oil.test,type ="class")
prune_tree.pred = predict(prune.tree, oil.test, type = "class")
Classifcation_tree_acc = mean(tree.pred == oil.test$unit)
prune_tree_acc = mean(prune_tree.pred == oil.test$unit)
# bagging
oil.bagging = bagging(unit~. , data = oil.train,nbagg = 100, coob = T,
control = rpart.control(minsplit = 2), cp = 0)
oil_bag_pred = predict(oil.bagging, newdata = oil.test)
bagging_acc = mean(oil.test$unit == oil_bag_pred)
# svm model
tune.out= tune(svm, unit~. ,data=oil.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
# best cost = 1
svmfit = svm( unit~., data=oil.train , type = "C-classification", kernel ="linear", cost =1)
svm_prediction = predict(svmfit, oil.test[, -6])
svm_acc = mean(prediction == oil.test$unit)
plot(svmfit, oil.test, x1~x2)
setwd("./Desktop/stat 441/Assignment4/")
library("rpart")
library("ipred")
library("tree")
library("e1071")
library("gbm")
oil.data = read.table("crude_oil.data", header=T,na.strings=c(""))
# data size is only 56, split into test and training set
set.seed(1)
train.index = sample (1: nrow(oil.data ), 40)
oil.train = oil.data[train.index, ]
oil.test = oil.data[-train.index, ]
# simple tree classification
oil.tree = tree(as.factor(unit)~. , oil.data, subset = train.index)
# classification tree model has 4 nodes
plot(oil.tree)
text(oil.tree, pretty = 0)
# tree pruning
prune.tree =prune.misclass (oil.tree ,best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred=predict(oil.tree, oil.test,type ="class")
prune_tree.pred = predict(prune.tree, oil.test, type = "class")
Classifcation_tree_acc = mean(tree.pred == oil.test$unit)
prune_tree_acc = mean(prune_tree.pred == oil.test$unit)
# bagging
oil.bagging = bagging(unit~. , data = oil.train,nbagg = 100, coob = T,
control = rpart.control(minsplit = 2), cp = 0)
oil_bag_pred = predict(oil.bagging, newdata = oil.test)
bagging_acc = mean(oil.test$unit == oil_bag_pred)
# svm model
tune.out= tune(svm, unit~. ,data=oil.train, kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 0.00001)) )
# best cost = 1
svmfit = svm( unit~., data=oil.train , type = "C-classification", kernel ="linear", cost =1)
svm_prediction = predict(svmfit, oil.test[, -6])
svm_acc = mean(svm_prediction == oil.test$unit)
plot(svmfit, oil.test, x1~x2)
summary(oil.bagging)
summary(oil.bagging$mtrees)
summary(oil.bagging$comb)
summary(oil.bagging$call)
summary(oil.bagging$y)
plot(oil.bagging)
plot(oil.bagging$mtrees)
plot(oil.bagging, oil.test)
plot(oil.bagging, oil.train)
plot(oil.bagging)
plot(svmfit, oil.test, x4~x4.1)
plot(svmfit, oil.test, x4~x1)
plot(svmfit, oil.test, x4.1~x1)
plot(svmfit, oil.test, x4.1~x4)
